{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Lexical Semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will use NLTK to access WordNet, look at some senses and lexical relations, and find paths between words. First, let's load NLTK and make sure WordNet is accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f5046eb0e1b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwordnet\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadme\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"eng\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "print(wn.readme(lang=\"eng\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in lecture, the main nodes in WordNet are synsets, not words. Given any word, we can access relevant synsets using the synsets commands. We can optionally limit to a particular word category (n = noun, v = verb, a = adjective, r = adverb). For each of the synsets of the word type \"class\", let's look at their definition, their corresponding lemmas, an example of their usage, and their hypernyms (often only one, but can be multiple)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book.n.01\n",
      "a written work or composition that has been published (printed on pages bound together)\n",
      "['book']\n",
      "['I am reading a good book on economics']\n",
      "[Synset('publication.n.01')]\n",
      "-------\n",
      "book.n.02\n",
      "physical objects consisting of a number of pages bound together\n",
      "['book', 'volume']\n",
      "['he used a large book as a doorstop']\n",
      "[Synset('product.n.02')]\n",
      "-------\n",
      "record.n.05\n",
      "a compilation of the known facts regarding something or someone\n",
      "['record', 'record_book', 'book']\n",
      "[\"Al Smith used to say, `Let's look at the record'\", 'his name is in all the record books']\n",
      "[Synset('fact.n.02')]\n",
      "-------\n",
      "script.n.01\n",
      "a written version of a play or other dramatic composition; used in preparing for a performance\n",
      "['script', 'book', 'playscript']\n",
      "[]\n",
      "[Synset('dramatic_composition.n.01')]\n",
      "-------\n",
      "ledger.n.01\n",
      "a record in which commercial accounts are recorded\n",
      "['ledger', 'leger', 'account_book', 'book_of_account', 'book']\n",
      "['they got a subpoena to examine our books']\n",
      "[Synset('record.n.07')]\n",
      "-------\n",
      "book.n.06\n",
      "a collection of playing cards satisfying the rules of a card game\n",
      "['book']\n",
      "[]\n",
      "[Synset('collection.n.01')]\n",
      "-------\n",
      "book.n.07\n",
      "a collection of rules or prescribed standards on the basis of which decisions are made\n",
      "['book', 'rule_book']\n",
      "['they run things by the book around here']\n",
      "[Synset('collection.n.01')]\n",
      "-------\n",
      "koran.n.01\n",
      "the sacred writings of Islam revealed by God to the prophet Muhammad during his life at Mecca and Medina\n",
      "['Koran', 'Quran', \"al-Qur'an\", 'Book']\n",
      "[]\n",
      "[]\n",
      "-------\n",
      "bible.n.01\n",
      "the sacred writings of the Christian religions\n",
      "['Bible', 'Christian_Bible', 'Book', 'Good_Book', 'Holy_Scripture', 'Holy_Writ', 'Scripture', 'Word_of_God', 'Word']\n",
      "['he went to carry the Word to the heathen']\n",
      "[Synset('sacred_text.n.01')]\n",
      "-------\n",
      "book.n.10\n",
      "a major division of a long written composition\n",
      "['book']\n",
      "['the book of Isaiah']\n",
      "[Synset('section.n.01')]\n",
      "-------\n",
      "book.n.11\n",
      "a number of sheets (ticket or stamps etc.) bound together on one edge\n",
      "['book']\n",
      "['he bought a book of stamps']\n",
      "[Synset('product.n.02')]\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "for synset in wn.synsets(\"book\",\"n\"):\n",
    "    print(synset.name())\n",
    "    print(synset.definition())\n",
    "    print(synset.lemma_names())\n",
    "    print(synset.examples())\n",
    "    print(synset.hypernyms())\n",
    "    print(\"-------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here why WordNet is sometimes seen as too fine-grained, particularly for word sense disambiguation; several of these senses are closely related to each other in meaning. WordNet does not ditinguish between true homonyms, and instances of polysemy. In any case, once we know its name, we can access a particular synset with the synset command, and look at other relationships, such as hyponyms; Note that meronyms and holonyms come in three types: part, member or substance, though we'll only look at part here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('album.n.02'), Synset('coffee-table_book.n.01'), Synset('folio.n.03'), Synset('hardback.n.01'), Synset('journal.n.04'), Synset('notebook.n.01'), Synset('novel.n.02'), Synset('order_book.n.02'), Synset('paperback_book.n.01'), Synset('picture_book.n.01'), Synset('sketchbook.n.01')]\n",
      "[Synset('binding.n.05'), Synset('fore_edge.n.01'), Synset('spine.n.04')]\n",
      "[Synset('text.n.01')]\n"
     ]
    }
   ],
   "source": [
    "print(wn.synset(\"book.n.02\").hyponyms())\n",
    "print(wn.synset(\"book.n.02\").part_meronyms())\n",
    "print(wn.synset(\"book.n.10\").part_holonyms()) # \"book\" meaning a division of a text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each synset has a set of lemmas associated with it. Since antonyms are often specific to the word form, they are defined on lemmas, not synsets. Another function, derivationally_related_forms gives other lemmas which are related by derivational morphology, though this is not comprehensive. Finally, lemmas have a count associated with them, derived from a sense tagged corpus: these can be used to identify which senses of a word are more common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('happy.a.01')\n",
      "[Lemma('unhappy.a.01.unhappy')]\n",
      "[Lemma('happiness.n.02.happiness'), Lemma('happiness.n.01.happiness')]\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "print(wn.synsets(\"happy\")[0])\n",
    "print(wn.synsets(\"happy\")[0].lemmas()[0].antonyms())\n",
    "print(wn.synsets(\"happy\")[0].lemmas()[0].derivationally_related_forms())\n",
    "print(wn.synsets(\"happy\")[0].lemmas()[0].count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the basic similarity measures mentioned in class (and several others) are available in the NLTK WordNet interface, as are other functions which are used in their derivation. For similarity metrics which require information content, we can load statistics from available corpora (the SEMCOR and Brown corpora are popular options)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet_ic to\n",
      "[nltk_data]     /Users/tcohn/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet_ic.zip.\n",
      "0.3333333333333333\n",
      "0.875\n",
      "0.5763952661933001\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet_ic\n",
    "import nltk\n",
    "nltk.download('wordnet_ic')\n",
    "\n",
    "print(wn.synset(\"book.n.02\").path_similarity(wn.synset(\"newspaper.n.03\")))\n",
    "print(wn.synset(\"book.n.02\").wup_similarity(wn.synset(\"newspaper.n.03\")))\n",
    "\n",
    "semcor_ic = wordnet_ic.ic('ic-semcor.dat')\n",
    "\n",
    "print(wn.synset(\"book.n.02\").lin_similarity(wn.synset(\"newspaper.n.03\"),semcor_ic))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, they are somewhat opaque in their operation, and only work on synsets. Let's create a version of basic path distance which doesn't require you to select a specific synset in advance, and shows you the exact path through the WordNet heirarchy that the score is based on. There are many ways to do this, we'll do it in a fairly clear but not entirely optimal way. First, given a set of synsets, let's get a dictionary where the keys correspond to all hypernym synsets, and the values are the next step below on the shortest past to one of the initial synsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Synset('publication.n.01'): Synset('book.n.01'), Synset('product.n.02'): Synset('book.n.02'), Synset('fact.n.02'): Synset('record.n.05'), Synset('dramatic_composition.n.01'): Synset('script.n.01'), Synset('record.n.07'): Synset('ledger.n.01'), Synset('collection.n.01'): Synset('book.n.06'), Synset('sacred_text.n.01'): Synset('bible.n.01'), Synset('section.n.01'): Synset('book.n.10'), Synset('document.n.03'): Synset('record.n.07'), Synset('group.n.01'): Synset('collection.n.01'), Synset('creation.n.02'): Synset('product.n.02'), Synset('work.n.02'): Synset('publication.n.01'), Synset('writing.n.02'): Synset('sacred_text.n.01'), Synset('music.n.01'): Synset('section.n.01'), Synset('information.n.01'): Synset('fact.n.02'), Synset('abstraction.n.06'): Synset('group.n.01'), Synset('message.n.02'): Synset('information.n.01'), Synset('communication.n.02'): Synset('document.n.03'), Synset('auditory_communication.n.01'): Synset('music.n.01'), Synset('written_communication.n.01'): Synset('writing.n.02'), Synset('artifact.n.01'): Synset('creation.n.02'), Synset('whole.n.02'): Synset('artifact.n.01'), Synset('entity.n.01'): Synset('abstraction.n.06'), Synset('object.n.01'): Synset('whole.n.02'), Synset('physical_entity.n.01'): Synset('object.n.01')}\n"
     ]
    }
   ],
   "source": [
    "def get_hypernym_path_dict(synsets):\n",
    "    hypernym_dict = {}\n",
    "    synsets_to_expand = synsets\n",
    "    while synsets_to_expand:\n",
    "        new_synsets_to_expand = set()\n",
    "        for synset in synsets_to_expand:\n",
    "            for hypernym in synset.hypernyms():\n",
    "                if hypernym not in hypernym_dict:  # this ensures we get the shortest path\n",
    "                    hypernym_dict[hypernym] = synset\n",
    "                    new_synsets_to_expand.add(hypernym)\n",
    "        synsets_to_expand = new_synsets_to_expand\n",
    "    return hypernym_dict\n",
    "        \n",
    "        \n",
    "hypernym_dict = get_hypernym_path_dict(wn.synsets(\"book\",\"n\"))\n",
    "print(hypernym_dict)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a way to build the path using this information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('physical_entity.n.01'), Synset('object.n.01'), Synset('whole.n.02'), Synset('artifact.n.01'), Synset('creation.n.02'), Synset('product.n.02'), Synset('book.n.02')]\n"
     ]
    }
   ],
   "source": [
    "def get_path_using_hypernym_dict(hypernym,hypernym_dict,synsets):\n",
    "    path = [hypernym]\n",
    "    current_synset = hypernym_dict[hypernym]\n",
    "    while current_synset not in synsets:\n",
    "        path.append(current_synset)\n",
    "        current_synset =  hypernym_dict[current_synset]\n",
    "    path.append(current_synset)\n",
    "    return path\n",
    "    \n",
    "print(get_path_using_hypernym_dict(wn.synset('physical_entity.n.01'),hypernym_dict,wn.synsets(\"book\",\"n\")))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we can build ancestor dictionaries for each of the words, look at the intersection, and then find the shortest path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n",
      "[Synset('book.n.02'), Synset('product.n.02'), Synset('newspaper.n.03')]\n",
      "0.2\n",
      "[Synset('dog.n.01'), Synset('canine.n.02'), Synset('carnivore.n.01'), Synset('feline.n.01'), Synset('cat.n.01')]\n",
      "0.2\n",
      "[Synset('nickel.n.02'), Synset('coin.n.01'), Synset('coinage.n.01'), Synset('currency.n.01'), Synset('money.n.03')]\n",
      "0.09090909090909091\n",
      "[Synset('calculator.n.01'), Synset('expert.n.01'), Synset('person.n.01'), Synset('causal_agent.n.01'), Synset('physical_entity.n.01'), Synset('matter.n.03'), Synset('substance.n.07'), Synset('food.n.01'), Synset('nutriment.n.01'), Synset('dish.n.02'), Synset('pizza.n.01')]\n",
      "0.16666666666666666\n",
      "[Synset('film.n.05'), Synset('sheet.n.06'), Synset('artifact.n.01'), Synset('creation.n.02'), Synset('product.n.02'), Synset('movie.n.01')]\n"
     ]
    }
   ],
   "source": [
    "def get_shortest_path_between(word1,word2):\n",
    "    synsets1 = wn.synsets(word1)\n",
    "    synsets2 = wn.synsets(word2)\n",
    "    hypernym_dict1 = get_hypernym_path_dict(synsets1)\n",
    "    hypernym_dict2 = get_hypernym_path_dict(synsets2)\n",
    "    best_path = []\n",
    "    for hypernym in hypernym_dict1:\n",
    "        if hypernym in hypernym_dict2 and hypernym_dict1[hypernym] != hypernym_dict2[hypernym]:\n",
    "            path1 = get_path_using_hypernym_dict(hypernym,hypernym_dict1,synsets1)\n",
    "            path2 = get_path_using_hypernym_dict(hypernym,hypernym_dict2,synsets2)\n",
    "            if not best_path or len(path1) + len(path2) - 1 < len(best_path):\n",
    "                path1.reverse()\n",
    "                best_path = path1 + path2[1:]\n",
    "    return best_path\n",
    "\n",
    "path = get_shortest_path_between(\"book\",\"newspaper\")\n",
    "print(1.0/len(path))\n",
    "print(path)\n",
    "\n",
    "path = get_shortest_path_between(\"dog\",\"cat\")\n",
    "print(1.0/len(path))\n",
    "print(path)\n",
    "\n",
    "path = get_shortest_path_between(\"nickel\",\"money\")\n",
    "print(1.0/len(path))\n",
    "print(path)\n",
    "\n",
    "path = get_shortest_path_between(\"computer\",\"pizza\")\n",
    "print(1.0/len(path))\n",
    "print(path)\n",
    "\n",
    "path = get_shortest_path_between(\"film\",\"movie\")\n",
    "print(1.0/len(path))\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shortest path does not always correspond to the most obvious relationship between two words: for instance, newspaper and book are join as products (not reading materials), dog and cat by informal senses related to people, rather than animals. Using depth and information-content basic metrics can improve this situation. Another approach is to use the counts of lemmas to ignore rare senses. Note that doing all this for other metrics is somewhat different, because they are based on the idea of lowest common subsumer, which is not necessarily on the shortest path."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
